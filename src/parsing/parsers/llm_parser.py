"""
LLMè§£æå™¨

ä½¿ç”¨LLMæœåŠ¡è¿›è¡Œå†…å®¹è§£æçš„ç»Ÿä¸€å®ç°ã€‚
"""

import json
import logging
import os
import time
import uuid
from typing import Any, Dict, List, Optional, Tuple

# Get the logger instance at the module level
logger = logging.getLogger(__name__)

# Import json_repair
try:
    import json_repair

    # Test if it has the loads method
    if not hasattr(json_repair, "loads"):
        raise ImportError("json_repair found but 'loads' function is missing.")
    logger.info("json-repair library loaded successfully.")
except ImportError:
    json_repair = None
    logger.warning("json-repair library not found or invalid. Falling back to standard json parsing for structured data.")


from src.llm.service_factory import create_llm_service
from src.parsing.base_parser import BaseParser
from src.parsing.prompt_templates import get_prompt_template, get_system_prompt

# Import the new utility function
from src.utils.file_utils import ConfigError, get_data_path


class LLMParser(BaseParser):
    """
    LLMè§£æå™¨

    ä½¿ç”¨ä¸åŒçš„LLMæœåŠ¡è§£æå†…å®¹çš„ç»Ÿä¸€å®ç°ã€‚
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–LLMè§£æå™¨

        Args:
            config: é…ç½®å‚æ•°ï¼ŒåŒ…å«providerä¿¡æ¯
        """
        super().__init__(config)

        self.config = config or {}
        self.provider = self.config.get("provider", "openai")

        try:
            # åˆ›å»ºLLMæœåŠ¡å®ä¾‹
            self.llm_service = create_llm_service(self.provider, self.config)
        except Exception as e:
            logger.error(f"Failed to create LLM service for provider '{self.provider}': {e}", exc_info=True)
            raise  # Re-raise the exception

        # åˆå§‹åŒ–ç¼“å­˜ (You might want to implement actual caching later)
        self._cache = {}

    def _get_temp_dir(self, sub_dir: Optional[str] = None) -> str:
        """è·å–ç”¨äºLLMæ—¥å¿—çš„ä¸´æ—¶ç›®å½•ï¼Œç¡®ä¿å…¶å”¯ä¸€æ€§ã€‚

        Args:
            sub_dir: å¯é€‰çš„å­ç›®å½•åç§°ã€‚

        Returns:
            ä¸´æ—¶ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚

        Raises:
            ValueError: å¦‚æœæ— æ³•é€šè¿‡ get_data_path ç¡®å®šæ•°æ®ç›®å½•ã€‚
        """
        try:
            # Generate a unique directory path using timestamp/UUID
            timestamp_dir = f"{int(time.time())}_{uuid.uuid4().hex[:8]}"
            final_sub_dir = os.path.join(sub_dir or "llm_logs", timestamp_dir)

            # Use get_data_path to construct the full path within the configured data/temp area
            base_temp_path = get_data_path("temp", final_sub_dir)

            if not base_temp_path:
                raise ValueError("`get_data_path` failed to return a valid path for the temporary directory.")

            # Ensure the directory exists
            os.makedirs(base_temp_path, exist_ok=True)
            logger.debug(f"Ensured temporary directory exists: {base_temp_path}")
            return base_temp_path

        except Exception as e:
            logger.error(f"åˆ›å»ºæˆ–è·å–LLMä¸´æ—¶æ—¥å¿—ç›®å½•å¤±è´¥: {e}", exc_info=True)
            # Re-raise the exception to make the error visible
            raise ValueError(f"Failed to create or access temporary LLM log directory: {e}") from e

    def _save_log_file(self, directory: str, prefix: str, timestamp: int, content: str) -> None:
        """å®‰å…¨åœ°ä¿å­˜æ—¥å¿—æ–‡ä»¶ (directory is now absolute path from _get_temp_dir)"""
        try:
            # directory is already the absolute path including timestamp
            # Ensure it exists one last time before writing (in case fallback was used)
            # Although _get_temp_dir tries to create it, double-check
            os.makedirs(directory, exist_ok=True)
            filepath = os.path.join(directory, f"{prefix}_{timestamp}.txt")
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)
            logger.info(f"ğŸ“ å·²ä¿å­˜æ—¥å¿—æ–‡ä»¶åˆ°: {filepath}")
        except OSError as e:
            # Log which specific directory failed if possible
            logger.warning(f"âš ï¸ æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ {prefix} åˆ° {directory}: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ä¿å­˜æ—¥å¿—æ–‡ä»¶ {prefix}: {e}")

    def _prepare_request(self, content: str, content_type: str) -> Tuple[List[Dict[str, str]], str]:
        """å‡†å¤‡LLMè¯·æ±‚æ‰€éœ€çš„æ¶ˆæ¯å’Œæ—¥å¿—å†…å®¹"""
        logger.debug(f"Preparing request for content_type: {content_type}")
        prompt_template = get_prompt_template(content_type)
        system_prompt = get_system_prompt(content_type)
        try:
            prompt = prompt_template.format(content=content)
        except KeyError as e:
            logger.error(f"æç¤ºæ¨¡æ¿ '{content_type}' æ ¼å¼åŒ–é”™è¯¯ï¼Œç¼ºå°‘é”®: {e}. Content: {content[:100]}...")
            raise ValueError(f"Prompt template formatting error for type '{content_type}': Missing key {e}") from e

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ]

        request_log_content = (
            f"Content Type: {content_type}\n\n"
            f"System Prompt:\n{system_prompt}\n\n"
            f"User Prompt:\n{prompt}\n\n"
            f"Original Content Snippet:\n{content[:500]}...\n"  # Log only a snippet
        )

        return messages, request_log_content

    async def _execute_llm_call(self, messages: List[Dict[str, str]]) -> str:
        """æ‰§è¡ŒLLMè°ƒç”¨å¹¶æå–åŸå§‹å“åº”æ–‡æœ¬"""
        logger.info("ğŸš€ å¼€å§‹è°ƒç”¨LLMæœåŠ¡...")
        try:
            response = await self.llm_service.chat_completion(messages)
            logger.info("âœ… LLMæœåŠ¡è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise  # Re-raise to be caught by the main parse_text method

        # Extract raw text based on provider response structure
        result_text = ""
        try:
            if hasattr(response, "choices") and response.choices and hasattr(response.choices[0], "message"):
                result_text = response.choices[0].message.content or ""  # Handle potential None
            elif isinstance(response, dict) and "choices" in response and len(response["choices"]) > 0:
                message_content = response["choices"][0].get("message", {}).get("content", "")
                result_text = message_content if isinstance(message_content, str) else ""
            else:
                logger.warning(f"æ— æ³•è¯†åˆ«çš„LLMå“åº”æ ¼å¼: {type(response)}ã€‚å°è¯•å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚")
                result_text = str(response)
        except (IndexError, KeyError, AttributeError, TypeError) as e:
            logger.warning(f"ä»LLMå“åº”ä¸­æå–æ–‡æœ¬æ—¶å‡ºé”™: {e}. Response: {str(response)[:200]}...")
            result_text = str(response)  # Fallback to string representation

        logger.info(f"ğŸ“¥ è·å–åˆ°LLMå“åº”ï¼ŒåŸå§‹æ–‡æœ¬é•¿åº¦: {len(result_text)}")
        logger.debug(f"Raw LLM Response Snippet: {result_text[:200]}...")
        return result_text

    def _parse_structured_response(self, result_text: str, content_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨json-repair (ä¼˜å…ˆ) æˆ–æ ‡å‡†åº“è§£æç»“æ„åŒ–å“åº” (workflow/roadmap)"""

        # 1. å°è¯•ä½¿ç”¨ json-repair (å¦‚æœå¯ç”¨)
        if json_repair:
            logger.debug(f"Attempting to parse with json_repair for {content_type}...")
            try:
                # Use json_repair.loads which attempts to fix and parse
                parsed_data = json_repair.loads(result_text)

                if isinstance(parsed_data, dict):
                    logger.info(f"âœ… æˆåŠŸä½¿ç”¨ json_repair è§£æå“åº”ä¸ºå­—å…¸å¯¹è±¡ ({content_type})")
                    return {
                        "success": True,
                        "content_type": content_type,
                        "content": parsed_data,
                        "raw_response": result_text,
                    }
                else:
                    # Handle case where repair succeeds but result isn't a dict
                    logger.warning(f"json_repair è§£ææˆåŠŸï¼Œä½†ç»“æœä¸æ˜¯å­—å…¸ç±»å‹: {type(parsed_data)}. Snippet: {str(parsed_data)[:100]}...")
                    # Fall through to standard JSON parsing or failure

            except ValueError as e:  # json_repair raises ValueError on failure
                logger.warning(f"âš ï¸ json_repair æ— æ³•ä¿®å¤æˆ–è§£æå“åº” ({content_type}). Error: {e}. å°†å°è¯•æ ‡å‡† JSON è§£æã€‚")
            except Exception as e:  # Catch any other unexpected errors from json_repair
                logger.warning(f"âš ï¸ ä½¿ç”¨ json_repair è§£ææ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ ({content_type}): {e}. å°†å°è¯•æ ‡å‡† JSON è§£æã€‚", exc_info=True)

        # 2. å¦‚æœ json-repair ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œå°è¯•æ ‡å‡† json.loads
        logger.debug(f"Attempting standard JSON parse for {content_type}...")
        try:
            parsed_data = json.loads(result_text)
            logger.info("âœ… æˆåŠŸä½¿ç”¨æ ‡å‡† json.loads è§£æå“åº”ä¸ºJSONå¯¹è±¡")
            return {
                "success": True,
                "content_type": content_type,
                "content": parsed_data,
                "raw_response": result_text,
            }
        except json.JSONDecodeError as e:
            error_msg = f"æ— æ³•å°†LLMå“åº”è§£æä¸ºJSON ({content_type}). æ ‡å‡†åº“é”™è¯¯: {e}"
            logger.error(error_msg)
            # Fall through to return failure

        # 3. å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        final_error_message = f"æ— æ³•å°†LLMå“åº”è§£æä¸ºJSON ({content_type})ï¼Œå·²å°è¯• json_repair (å¦‚æœå¯ç”¨) å’Œæ ‡å‡†åº“ã€‚"
        logger.error(final_error_message)
        return {
            "success": False,
            "error": final_error_message,
            "content_type": content_type,
            "content_preview": result_text[:300] + "...",
            "raw_response": result_text,
        }

    # --- Specific Processors (Kept for simple non-JSON types, could be refactored further) ---
    # These typically operate on the *original* content, not the LLM response usually.
    # Pass result_text if they need to analyze the LLM output instead/as well.

    def _process_rule_response(self, content: str, result_text: str) -> Dict[str, Any]:
        """å¤„ç†è§„åˆ™å“åº” (Simple extraction from original content)"""
        logger.debug("Processing rule response (simple extraction)...")
        front_matter = {}
        markdown_content = content
        title = ""
        try:
            if content.startswith("---"):
                end_index = content.find("---", 3)
                if end_index != -1:
                    front_matter_text = content[3:end_index].strip()
                    for line in front_matter_text.split("\n"):
                        if ":" in line:
                            key, value = line.split(":", 1)
                            front_matter[key.strip()] = value.strip()
                    markdown_content = content[end_index + 3 :].strip()

            lines = markdown_content.split("\n")
            for line in lines:
                if line.startswith("# "):
                    title = line[2:].strip()
                    break
        except Exception as e:
            logger.warning(f"Simple rule processing failed: {e}")
            # Return basic structure even if parsing fails
            markdown_content = content  # Reset to original if parsing failed
            title = "Unknown Title"
            front_matter = {}

        return {
            "success": True,
            "content_type": "rule",
            "front_matter": front_matter,
            "title": title,
            "content": markdown_content,
            "metadata": {
                "title": title,
                "type": front_matter.get("type", "unknown"),
                "description": front_matter.get("description", ""),
                "tags": front_matter.get("tags", "").split(",") if front_matter.get("tags") else [],
            },
            "raw_response": result_text,  # Include raw LLM response
        }

    def _process_document_response(self, content: str, result_text: str) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£å“åº” (Simple extraction from original content)"""
        logger.debug("Processing document response (simple extraction)...")
        title = ""
        headings = []
        lines = content.split("\n")
        try:
            for line in lines:
                if line.startswith("# "):
                    title = line[2:].strip()
                    break
            for line in lines:
                if line.startswith("#"):
                    level = 0
                    for char in line:  # Safer way to count leading '#'
                        if char == "#":
                            level += 1
                    else:
                        break
            if line[level:].startswith(" "):  # Ensure space after #
                heading_text = line[level:].strip()
                headings.append({"level": level, "text": heading_text})
        except Exception as e:
            logger.warning(f"Simple document processing failed: {e}")
            title = "Unknown Title"
            headings = []

        return {
            "success": True,
            "content_type": "document",
            "title": title,
            "content": content,
            "structure": {"headings": headings},
            "metadata": {
                "title": title,
                "word_count": len(content.split()),
                "line_count": len(lines),
            },
            "raw_response": result_text,
        }

    def _process_generic_response(self, content: str, result_text: str) -> Dict[str, Any]:
        """å¤„ç†é€šç”¨å“åº” (Returns raw LLM result)"""
        logger.debug("Processing generic response...")
        return {
            "success": True,
            "content_type": "generic",
            "content": content,  # Original input content
            "result": result_text,  # Raw LLM output
            "metadata": {
                "line_count": len(content.split("\n")),
                "word_count": len(content.split()),
                "char_count": len(content),
            },
            "raw_response": result_text,
        }

    # --- Main Orchestration Method ---
    async def parse_text(self, content: str, content_type: Optional[str] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæœåŠ¡è§£ææ–‡æœ¬å†…å®¹ (Orchestration Method)

        Args:
            content: å¾…è§£æçš„æ–‡æœ¬å†…å®¹
            content_type: å†…å®¹ç±»å‹ï¼Œå¦‚\'rule\'ã€\'document\'ç­‰

        Returns:
            è§£æç»“æœ
        """
        content_type = content_type or "generic"
        timestamp = int(time.time())
        # Use microseconds for log dir uniqueness if many requests happen in one second
        timestamp_dir = self._get_temp_dir(f"llm_logs/{content_type}")
        result_text = ""  # Initialize for potential use in exception handlers

        try:
            # 1. Prepare Request
            messages, request_log_content = self._prepare_request(content, content_type)
            self._save_log_file(timestamp_dir, "request", timestamp, request_log_content)

            # 2. Execute LLM Call
            result_text = await self._execute_llm_call(messages)
            self._save_log_file(timestamp_dir, "response", timestamp, result_text)

            # 3. Parse/Process Response based on type
            if content_type in ["workflow", "roadmap"]:
                # Use the dedicated parsing method with json-repair
                return self._parse_structured_response(result_text, content_type)
            else:
                # Fallback to specific simple processors or generic one
                logger.debug(f"Using simple/generic processor for content type: {content_type}")
                content_processors = {
                    "rule": self._process_rule_response,
                    "document": self._process_document_response,
                    # Add other specific processors if they exist
                }
                # Use generic processor if no specific one is found
                processor = content_processors.get(content_type, self._process_generic_response)
                return processor(content, result_text)

        except Exception as e:
            # General exception handling for call or parsing logic errors
            error_msg = f"LLMè§£ææµç¨‹å¤±è´¥ ({content_type}): {str(e)}"
            logger.error(error_msg, exc_info=True)
            # Save error log
            error_log_content = f"LLMè§£æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}\n\n"
            error_log_content += f"Content Type: {content_type}\n\n"
            if result_text:
                error_log_content += f"è·å–åˆ°çš„å“åº” (å¯èƒ½ä¸å®Œæ•´):\n{result_text}"
            else:
                error_log_content += "æœªèƒ½è·å–ä»»ä½•å“åº”æˆ–åœ¨è·å–å“åº”å‰å‘ç”Ÿé”™è¯¯ã€‚"
            self._save_log_file(timestamp_dir, f"llm_error", timestamp, error_log_content)

            return {
                "success": False,
                "error": error_msg,
                "content_type": content_type,
                "content_preview": content[:100] + "...",
                "raw_response": result_text if result_text else "è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œæœªèƒ½è·å–å“åº”",
            }

    # --- Sync Wrapper (Kept for compatibility) ---
    def parse(self, content: str, content_type: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥æ–¹æ³•ï¼Œè§£ææ–‡æœ¬å†…å®¹

        è¿™æ˜¯ä¸€ä¸ªé€‚é…æ–¹æ³•ï¼Œå…è®¸åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨LLMè§£æå™¨ã€‚
        """
        import asyncio

        # Correct way to run async from sync: use asyncio.run() if possible,
        # or manage event loop if running inside another loop.
        # Assuming this parse() is called from a top-level sync context.
        try:
            # Use asyncio.run() for simplicity if possible.
            # If this needs to integrate with an existing loop (e.g., FastAPI),
            # the calling code needs to handle the await differently.
            logger.info("è¿è¡Œå¼‚æ­¥ LLM è§£æ (Sync Wrapper)...")
            result = asyncio.run(self.parse_text(content, content_type))
            return result
        except RuntimeError as e:
            # Handle cases where asyncio.run() can't be used (e.g., nested loops)
            if "cannot be called from a running event loop" in str(e):
                logger.error("åŒæ­¥ `parse` æ–¹æ³•ä¸èƒ½ä»æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨ã€‚è¯·ç›´æ¥ `await parse_text`ã€‚")
                return {"success": False, "error": "Async context error: Cannot run nested event loops."}
            else:
                logger.error(f"âŒ LLMè§£æå¤±è´¥ (Sync Wrapper - Runtime Error): {str(e)}", exc_info=True)
                # ... (log error to file?) ...
                return {
                    "success": False,
                    "error": f"LLMè§£æå¤±è´¥ (Runtime Error): {str(e)}",
                    "content_type": content_type or "generic",
                    "content_preview": content[:100] + "...",
                }
        except Exception as e:
            logger.error(f"âŒ LLMè§£æå¤±è´¥ (Sync Wrapper - General Error): {str(e)}", exc_info=True)
            # ... (log error to file?) ...
            # ... (error return structure) ...
            return {
                "success": False,
                "error": f"LLMè§£æå¤±è´¥: {str(e)}",
                "content_type": content_type or "generic",
                "content_preview": content[:100] + "...",
                "original_error": str(e),
            }
